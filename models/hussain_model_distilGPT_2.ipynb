{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3_JBKkw9pkj",
        "outputId": "e0e1ce4b-5eba-4fab-9adc-b9d112b04ac4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.2)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "# Install necessary libraries\n",
        "!pip install transformers datasets torch\n",
        "\n",
        "# Import necessary modules\n",
        "from transformers import AutoModelForCausalLM, Trainer, TrainingArguments, AutoTokenizer\n",
        "from datasets import load_dataset, Dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXTiixuC-Q4_"
      },
      "outputs": [],
      "source": [
        "# df1 = pd.read_excel(\"data1.xlsx\")\n",
        "# df2 = pd.read_excel(\"data2.xlsx\")\n",
        "# df = pd.concat([df1,df2],axis=0)\n",
        "\n",
        "df = pd.read_excel(\"data.xlsx\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lW3_-TM-mzR"
      },
      "outputs": [],
      "source": [
        "# Combine 'scenario' and 'witnesses' as inputs and 'judgment' as target\n",
        "df[\"scenario\"] = df[\"scenario\"].fillna(\"\").astype(str)\n",
        "df[\"witnesses\"] = df[\"witnesses\"].fillna(\"\").astype(str)\n",
        "df[\"judgment\"] = df[\"judgment\"].fillna(\"\").astype(str)\n",
        "inputs = df[\"scenario\"] + \" \" + df[\"witnesses\"]\n",
        "targets = df[\"judgment\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WaxxV14L-n4b"
      },
      "outputs": [],
      "source": [
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "test_dataset = Dataset.from_pandas(test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XBcwnrVO-sA0"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "# Login with your Hugging Face token\n",
        "login(\"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\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5P9zIj6-uUX",
        "outputId": "6f2ca41a-54d4-4f71-ee2a-522e4d5cec5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "# # Load tokenizer and model for distilGPT-2\n",
        "# model_name = \"distilgpt2\"  # Lightweight GPT-2 model\n",
        "# tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "# model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the path where the model is saved in Google Drive\n",
        "output_dir = \"/content/drive/MyDrive/saved_model\"  # Replace with your saved model path\n",
        "\n",
        "# Load the saved pre-trained model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(output_dir)\n",
        "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "7507c73a3562457eb9b1ce308cba2c61",
            "4e23fc4557744d9b8dbd8d59f62fcd1d",
            "6b6599ceb1854c11a66e9f075e64eae2",
            "3e809ca8f50042248488f1cd3c5e17e0",
            "817ed544d97b4afd96bb910db848aa63",
            "442865e87a324b198c2a470f8aeec477",
            "76651a5ebb9943c5b2a21cd735315e51",
            "702d9574bf6d42e99064554d1235a619",
            "7d39cf9543844ce4bbf1d2c84975144a",
            "260930c9ec9440bea12d925c0d74d1ec",
            "258a801df2f44733b9767f378d301da0",
            "47818ceea7e24e6ab956c830b80a5582",
            "dfb95ef2ed7f4dbead4fbfafcc717db5",
            "a4ac135734cf48a598c5632bb97f168a",
            "b5726d1501c543e884f7923caf481059",
            "91b245fb892449d782096de594cb558b",
            "4326df0f5a6c42ce8bbebd6d56787d04",
            "c29889c88e964a418008b923112232ff",
            "6bbbd1d5bba74425a8f5a767e246a8e9",
            "ff2d0146e07e438dab88b9b182bb0a87",
            "b3ef609721f449508ba4f60a53bcedcd",
            "60a3f612b2d149b8819feea1aac31ad8"
          ]
        },
        "id": "Rj3_muoA_MSh",
        "outputId": "a60ca0ca-481f-4580-9664-6d6d9ef16ba9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/801 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7507c73a3562457eb9b1ce308cba2c61"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/201 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "47818ceea7e24e6ab956c830b80a5582"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "from datasets import DatasetDict\n",
        "from google.colab import drive\n",
        "\n",
        "# Define the path where the model is saved in Google Drive\n",
        "output_dir = \"/content/drive/MyDrive/saved_model\"  # Replace with your saved model path\n",
        "\n",
        "# Load the saved tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
        "\n",
        "# Set padding token\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Use eos_token as pad_token\n",
        "\n",
        "max_length = 1024\n",
        "\n",
        "# Function to preprocess dataset for distilgpt2\n",
        "def preprocess_data_distilgpt2(examples):\n",
        "    \"\"\"\n",
        "    Preprocesses the dataset by combining 'scenario', 'witnesses', and 'judgment'\n",
        "    into a single input sequence for autoregressive modeling.\n",
        "    \"\"\"\n",
        "    # Combine 'scenario', 'witnesses', and 'judgment' into a single string\n",
        "    inputs = [\n",
        "        f\"Scenario: {scenario} Witnesses: {witnesses} Judgment: {judgment}\"\n",
        "        for scenario, witnesses, judgment in zip(examples[\"scenario\"], examples[\"witnesses\"], examples[\"judgment\"])\n",
        "    ]\n",
        "\n",
        "    # Tokenize the combined input\n",
        "    model_inputs = tokenizer(\n",
        "        inputs,\n",
        "        max_length=max_length,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",  # Pad sequences to max_length\n",
        "        return_tensors=\"np\",  # Return NumPy arrays for compatibility with datasets\n",
        "    )\n",
        "\n",
        "    # Labels are the same as the input for GPT-style models\n",
        "    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].copy()\n",
        "    return model_inputs\n",
        "\n",
        "# Apply preprocessing to datasets\n",
        "train_dataset = train_dataset.map(preprocess_data_distilgpt2, batched=True)\n",
        "test_dataset = test_dataset.map(preprocess_data_distilgpt2, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ulM0t_og_9N"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, Trainer, TrainingArguments, AutoTokenizer\n",
        "from datasets import DatasetDict\n",
        "from google.colab import drive\n",
        "\n",
        "# # Load tokenizer\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
        "\n",
        "# Define the path where the model is saved in Google Drive\n",
        "output_dir = \"/content/drive/MyDrive/saved_model\"  # Replace with your saved model path\n",
        "\n",
        "# Load the saved tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
        "\n",
        "# Define maximum sequence length\n",
        "max_length = 1024\n",
        "\n",
        "# Preprocessing function for dataset\n",
        "def preprocess_data_distilgpt2(examples):\n",
        "    \"\"\"\n",
        "    Preprocesses the dataset by combining 'scenario', 'witnesses', and 'judgment'\n",
        "    into a single sequence for autoregressive modeling.\n",
        "    \"\"\"\n",
        "    # Combine scenario, witnesses, and judgment into one input sequence\n",
        "    inputs = [\n",
        "        f\"Scenario: {scenario} Witnesses: {witnesses} Judgment: {judgment}\"\n",
        "        for scenario, witnesses, judgment in zip(examples[\"scenario\"], examples[\"witnesses\"], examples[\"judgment\"])\n",
        "    ]\n",
        "\n",
        "    # Tokenize the combined input\n",
        "    model_inputs = tokenizer(\n",
        "        inputs, max_length=max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # Labels are the same as the input IDs for GPT-style models\n",
        "    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].clone()\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "-biJJcb_hB6L",
        "outputId": "8fae27d8-1c30-4408-d271-37c496889c15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-ff12ade9e99b>:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.18.7"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241126_174500-5xb8the7</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ansabchaudhary8-habib-university/huggingface/runs/5xb8the7' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/ansabchaudhary8-habib-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/ansabchaudhary8-habib-university/huggingface' target=\"_blank\">https://wandb.ai/ansabchaudhary8-habib-university/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/ansabchaudhary8-habib-university/huggingface/runs/5xb8the7' target=\"_blank\">https://wandb.ai/ansabchaudhary8-habib-university/huggingface/runs/5xb8the7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='161' max='161' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [161/161 01:13, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.132651</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=161, training_loss=1.6125829471564441, metrics={'train_runtime': 162.3518, 'train_samples_per_second': 4.934, 'train_steps_per_second': 0.992, 'total_flos': 209298697224192.0, 'train_loss': 1.6125829471564441, 'epoch': 1.0})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, Trainer, TrainingArguments, AutoTokenizer\n",
        "from google.colab import drive\n",
        "import torch\n",
        "\n",
        "# Define the path where the model is saved in Google Drive\n",
        "output_dir = \"/content/drive/MyDrive/saved_model\"  # Replace with your saved model path\n",
        "\n",
        "# Load the saved pre-trained model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(output_dir)\n",
        "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
        "\n",
        "# Add a padding token if not already present\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    eval_strategy=\"epoch\",  # Updated deprecated argument\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=5,  # Adjust based on GPU memory\n",
        "    num_train_epochs=1,  # Adjust based on dataset size\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=2,\n",
        "    logging_dir=\"./logs\",\n",
        "    fp16=True,  # Enable mixed precision for faster training on GPUs\n",
        ")\n",
        "\n",
        "# Setup Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    data_collator=None,  # Use default data collator\n",
        "    tokenizer=tokenizer,  # Specify tokenizer for processing\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "trainer.train()\n",
        "\n",
        "# ef28151f89ca836cffd195b64d4695256c02f32d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0PvNMf7jcVZ",
        "outputId": "5153148b-6d03-4b01-88fd-1553fd8bddaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.9.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.26.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.11.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        }
      ],
      "source": [
        "!pip install evaluate scikit-learn\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, Trainer, TrainingArguments, AutoTokenizer\n",
        "import evaluate\n",
        "import pandas as pd\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Move model to the correct device (GPU if available, otherwise CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Function to predict judgment\n",
        "def predict_judgment(scenario, witnesses):\n",
        "    \"\"\"\n",
        "    Generates a judgment using the fine-tuned distilGPT-2 model.\n",
        "    \"\"\"\n",
        "    # Adjust the input format to focus on generating only the judgment\n",
        "    input_text = f\"Scenario: {scenario}\\nWitnesses: {witnesses}\\nJudgment: \"\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
        "\n",
        "    # Generate prediction\n",
        "    output_ids = model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_new_tokens=100,  # Specify the number of tokens to generate\n",
        "        num_beams=5,\n",
        "        no_repeat_ngram_size=3,\n",
        "        length_penalty=1.2,\n",
        "        early_stopping=True,\n",
        "        pad_token_id=tokenizer.pad_token_id  # Ensure padding is handled\n",
        "    )\n",
        "    # Decode only the generated part (after \"Judgment:\")\n",
        "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    return generated_text.split(\"Judgment:\")[-1].strip()\n",
        "\n",
        "    # Evaluate on the test set\n",
        "results = []\n",
        "\n",
        "y_true = []  # For F1 score computation\n",
        "y_pred = []  # For F1 score computation\n",
        "\n",
        "for i in range(len(test_df)):\n",
        "    scenario = test_df.iloc[i][\"scenario\"]\n",
        "    witnesses = test_df.iloc[i][\"witnesses\"]\n",
        "    reference_judgment = test_df.iloc[i][\"judgment\"]\n",
        "    predicted_judgment = predict_judgment(scenario, witnesses)\n",
        "\n",
        "\n",
        "    # Add reference and prediction to the lists for F1 score computation\n",
        "    y_true.append(reference_judgment.split())  # Tokenize reference\n",
        "    y_pred.append(predicted_judgment.split())  # Tokenize prediction\n",
        "\n",
        "    results.append({\n",
        "        \"Scenario\": scenario,\n",
        "        \"Witnesses\": witnesses,\n",
        "        \"Reference Judgment\": reference_judgment,\n",
        "        \"Predicted Judgment\": predicted_judgment\n",
        "    })\n",
        "\n",
        "# ---------------------------------------------------------------------------------------\n",
        "\n",
        "# !pip install rouge-score datasets\n",
        "# !pip install evaluate\n",
        "# import torch\n",
        "# import evaluate\n",
        "# import torch\n",
        "\n",
        "# # Move model to the correct device\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# model.to(device)\n",
        "\n",
        "# rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "# # Function to predict judgment\n",
        "# def predict_judgment(scenario, witnesses):\n",
        "#     \"\"\"\n",
        "#     Generates a judgment using the fine-tuned model.\n",
        "#     \"\"\"\n",
        "#     input_text = f\"Scenario: {scenario} Witnesses: {witnesses}\"\n",
        "#     inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
        "\n",
        "#     # Generate judgment prediction with max_new_tokens\n",
        "#     summary_ids = model.generate(\n",
        "#         inputs[\"input_ids\"],\n",
        "#         max_new_tokens=100,  # Generate up to 100 new tokens\n",
        "#         num_beams=5,\n",
        "#         no_repeat_ngram_size=3,\n",
        "#         length_penalty=1.2,\n",
        "#         early_stopping=True,\n",
        "#         pad_token_id=tokenizer.pad_token_id  # Ensure padding is handled correctly\n",
        "#     )\n",
        "#     return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "# # Evaluate on the test set\n",
        "# results = []\n",
        "# for i in range(len(test_df)):\n",
        "#     scenario = test_df.iloc[i][\"scenario\"]\n",
        "#     witnesses = test_df.iloc[i][\"witnesses\"]\n",
        "#     reference_judgment = test_df.iloc[i][\"judgment\"]\n",
        "#     predicted_judgment = predict_judgment(scenario, witnesses)\n",
        "#     rouge.add(prediction=predicted_judgment, reference=reference_judgment)\n",
        "#     results.append({\n",
        "#         \"Scenario\": scenario,\n",
        "#         \"Witnesses\": witnesses,\n",
        "#         \"Reference Judgment\": reference_judgment,\n",
        "#         \"Predicted Judgment\": predicted_judgment\n",
        "#     })\n",
        "\n",
        "# ----------------------------------------------------------------------------------------\n",
        "\n",
        "# !pip install rouge-score datasets\n",
        "# !pip install evaluate scikit-learn\n",
        "# import torch\n",
        "# from sklearn.metrics import f1_score\n",
        "\n",
        "# # Move model to the correct device\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# model.to(device)\n",
        "\n",
        "# # Function to predict judgment\n",
        "# def predict_judgment(scenario, witnesses):\n",
        "#     \"\"\"\n",
        "#     Generates a judgment using the fine-tuned model.\n",
        "#     \"\"\"\n",
        "#     input_text = f\"Scenario: {scenario} Witnesses: {witnesses}\"\n",
        "#     inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
        "\n",
        "#     # Generate judgment prediction with max_new_tokens\n",
        "#     summary_ids = model.generate(\n",
        "#         inputs[\"input_ids\"],\n",
        "#         max_new_tokens=100,  # Generate up to 100 new tokens\n",
        "#         num_beams=5,\n",
        "#         no_repeat_ngram_size=3,\n",
        "#         length_penalty=1.2,\n",
        "#         early_stopping=True,\n",
        "#         pad_token_id=tokenizer.pad_token_id  # Ensure padding is handled correctly\n",
        "#     )\n",
        "#     return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "# # Evaluate on the test set\n",
        "# results = []\n",
        "# y_true = []  # For F1 score computation\n",
        "# y_pred = []  # For F1 score computation\n",
        "\n",
        "# for i in range(len(test_df)):\n",
        "#     scenario = test_df.iloc[i][\"scenario\"]\n",
        "#     witnesses = test_df.iloc[i][\"witnesses\"]\n",
        "#     reference_judgment = test_df.iloc[i][\"judgment\"]\n",
        "#     predicted_judgment = predict_judgment(scenario, witnesses)\n",
        "\n",
        "#     # Add reference and prediction to the lists for F1 score computation\n",
        "#     y_true.append(reference_judgment.split())  # Tokenize reference\n",
        "#     y_pred.append(predicted_judgment.split())  # Tokenize prediction\n",
        "\n",
        "#     results.append({\n",
        "#         \"Scenario\": scenario,\n",
        "#         \"Witnesses\": witnesses,\n",
        "#         \"Reference Judgment\": reference_judgment,\n",
        "#         \"Predicted Judgment\": predicted_judgment\n",
        "#     })"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Compute ROUGE scores\n",
        "# rouge_scores = rouge.compute()\n",
        "# print(\"ROUGE Scores:\", rouge_scores)\n",
        "\n",
        "# # Save results to Excel\n",
        "# results_df = pd.DataFrame(results)\n",
        "# results_df.to_excel(\"/content/predicted_judgments_distilgpt2.xlsx\", index=False)\n",
        "\n",
        "# print(\"Predictions saved to /content/predicted_judgments_distilgpt2.xlsx\")\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "\n",
        "# Token-based F1 computation: align predictions with references\n",
        "def calculate_f1_score(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Compute token-level F1 score by treating token matches as labels.\n",
        "    \"\"\"\n",
        "    # Convert token lists to binary labels for overlap comparison\n",
        "    y_true_binary = []\n",
        "    y_pred_binary = []\n",
        "\n",
        "    for true, pred in zip(y_true, y_pred):\n",
        "        # Combine true and predicted tokens to form a shared vocabulary\n",
        "        vocabulary = list(set(true + pred))\n",
        "\n",
        "        # Binary representation of presence in true and pred\n",
        "        y_true_binary.append([1 if token in true else 0 for token in vocabulary])\n",
        "        y_pred_binary.append([1 if token in pred else 0 for token in vocabulary])\n",
        "\n",
        "    # Flatten binary representations\n",
        "    flattened_true = np.concatenate(y_true_binary)\n",
        "    flattened_pred = np.concatenate(y_pred_binary)\n",
        "\n",
        "    # Compute weighted F1 score\n",
        "    return f1_score(flattened_true, flattened_pred, average=\"weighted\")\n",
        "\n",
        "# Compute F1 Score\n",
        "f1 = calculate_f1_score(y_true, y_pred)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "# Save results to Excel\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_excel(\"/content/predicted_judgments_distilgpt2.xlsx\", index=False)\n",
        "\n",
        "print(\"Predictions saved to /content/predicted_judgments_distilgpt2.xlsx\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_RfZvMKMKG_",
        "outputId": "feb8007d-ad0f-4ad6-c909-7ace1f4b71f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score: 0.1998000827057157\n",
            "Predictions saved to /content/predicted_judgments_distilgpt2.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# path = F\"/content/drive/MyDrive/saved_model\"\n",
        "\n",
        "# # Save the model's state_dict\n",
        "# torch.save(model.state_dict(), path)\n",
        "\n",
        "# print(f\"Model saved to {path}\")\n",
        "\n",
        "from google.colab import drive\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# # Define the path to save the model in Google Drive\n",
        "output_dir = \"/content/drive/MyDrive/saved_model\"\n",
        "\n",
        "# # Load the pre-trained or fine-tuned model and tokenizer\n",
        "# model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")  # Replace with your model name if fine-tuned\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")  # Replace with your tokenizer if different\n",
        "\n",
        "# Save the trained model and tokenizer\n",
        "model.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "print(f\"Model and tokenizer saved to {output_dir}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Ug_IjXWwMK86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f873b118-d6cd-435f-d694-3de3e1f4b463"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model and tokenizer saved to /content/drive/MyDrive/saved_model\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7507c73a3562457eb9b1ce308cba2c61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4e23fc4557744d9b8dbd8d59f62fcd1d",
              "IPY_MODEL_6b6599ceb1854c11a66e9f075e64eae2",
              "IPY_MODEL_3e809ca8f50042248488f1cd3c5e17e0"
            ],
            "layout": "IPY_MODEL_817ed544d97b4afd96bb910db848aa63"
          }
        },
        "4e23fc4557744d9b8dbd8d59f62fcd1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_442865e87a324b198c2a470f8aeec477",
            "placeholder": "​",
            "style": "IPY_MODEL_76651a5ebb9943c5b2a21cd735315e51",
            "value": "Map: 100%"
          }
        },
        "6b6599ceb1854c11a66e9f075e64eae2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_702d9574bf6d42e99064554d1235a619",
            "max": 801,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7d39cf9543844ce4bbf1d2c84975144a",
            "value": 801
          }
        },
        "3e809ca8f50042248488f1cd3c5e17e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_260930c9ec9440bea12d925c0d74d1ec",
            "placeholder": "​",
            "style": "IPY_MODEL_258a801df2f44733b9767f378d301da0",
            "value": " 801/801 [00:01&lt;00:00, 677.11 examples/s]"
          }
        },
        "817ed544d97b4afd96bb910db848aa63": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "442865e87a324b198c2a470f8aeec477": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76651a5ebb9943c5b2a21cd735315e51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "702d9574bf6d42e99064554d1235a619": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d39cf9543844ce4bbf1d2c84975144a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "260930c9ec9440bea12d925c0d74d1ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "258a801df2f44733b9767f378d301da0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "47818ceea7e24e6ab956c830b80a5582": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dfb95ef2ed7f4dbead4fbfafcc717db5",
              "IPY_MODEL_a4ac135734cf48a598c5632bb97f168a",
              "IPY_MODEL_b5726d1501c543e884f7923caf481059"
            ],
            "layout": "IPY_MODEL_91b245fb892449d782096de594cb558b"
          }
        },
        "dfb95ef2ed7f4dbead4fbfafcc717db5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4326df0f5a6c42ce8bbebd6d56787d04",
            "placeholder": "​",
            "style": "IPY_MODEL_c29889c88e964a418008b923112232ff",
            "value": "Map: 100%"
          }
        },
        "a4ac135734cf48a598c5632bb97f168a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6bbbd1d5bba74425a8f5a767e246a8e9",
            "max": 201,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ff2d0146e07e438dab88b9b182bb0a87",
            "value": 201
          }
        },
        "b5726d1501c543e884f7923caf481059": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b3ef609721f449508ba4f60a53bcedcd",
            "placeholder": "​",
            "style": "IPY_MODEL_60a3f612b2d149b8819feea1aac31ad8",
            "value": " 201/201 [00:00&lt;00:00, 577.95 examples/s]"
          }
        },
        "91b245fb892449d782096de594cb558b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4326df0f5a6c42ce8bbebd6d56787d04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c29889c88e964a418008b923112232ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6bbbd1d5bba74425a8f5a767e246a8e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff2d0146e07e438dab88b9b182bb0a87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b3ef609721f449508ba4f60a53bcedcd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60a3f612b2d149b8819feea1aac31ad8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}